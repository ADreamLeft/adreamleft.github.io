# 第一周
 1万个句子判断有没有技术，人工要800块钱，GPT只要20美元
 ChatGPT的最初优势：routine work更便宜。
为什么大模型有幻觉，怎么控制幻觉：
GPT-3比Transformer的区别，参数量变大1000倍
大语言模型的input：上下文窗口里所有字；output：下一个字
word2vec：google的算法，可以吧一个词变为向量，功能是可以让词加减
维度：1万多个维度
transformer：attention，MLP
词库法：计算积极词所占比例，不准确
大语言模型：GPT3需要175Billion
quantization（量化）：降低数字的精度
概念判断：5万个概念判断，符合概念，则加上概念对应的向量
基本正交才能重复出差别
Temperature：残差项设为零，越大的temperature残差越大。
Top P：设为1时输出概率最大的那个。
# 第二周
GPT-3 的文章：language models are few-shot learners
Few shot 是什么
GPT-3 的语料：common crawl（整个互联网的语料）
解决语料问题的思路：
- FinBERT：完全用金融语料训练出来的
- GPT + Finetuning（微调，LoRA）
- GPT + Few-shot
训练finBERT要的算力是 2，GPT需要7000左右
12288：把字转换为向量时空间的维度
模型变大的好处：
- 每个字的维度更多
GPT-2 到 3，参数变大
传统的翻译任务
BERT：
- pretraining
- fine-tuning
GPT：Zero-shot
few-shot：数学上只改变了模型的参数，计算的难度前置。
FinBERT效果好于GPT+few shot，两者均好于词库法
BERT区分于GPT：无方向性
BERT比GPT好的原因
- 考虑上下文关系
- 专门训练用于情感分析
BERT训练的过程：用空位符盖住15%的词，然后反推，调整矩阵参数
BERT来自Transformer分析，窗口只有512个词，只能分析句子不能分析段落。
窗口长度：
- GPT 2 1024
- GPT 3 2048
- GPT 3.5 4096
- GPT 4 8192 / 32768
- DeepSeek：宣称12万

长度上限达到之后会遗忘
标准的情感分析Prompt：
- 告诉模型任务是情感分析
- 给出output样式
- 给出例子
GPT会有一次finetuning，目的是让GPT成为你的助手。
COT和o1的关系是什么？Tree of Thought 是什么？o1是 COT还是TOT
神经网络：线性函数叠加非线性函数，实际发生了空间的扭曲
manifold hypothesis：参数化做不到，深度学习中间非线性的函数把空间扭曲以后高维世界，manifold的曲面反而变成低维了。
curse of dimensionality：当你的模型维度越高的时候。估计它就越难度？它不是线性增加的是指数增加。
# 第三周
 GPT以前的语言模型需要Pretrain和Finetuning吗？需要
 BERT都需要，GPT需要pretrain，但是不一定需要finetuning（原始用途不做，但是后来也有人这么做）
BERT 怎么训练的：预训练（填空题），Finetuning（特化情感分析的任务、不需要提示词）
中文情感分析：找中文的BERT；自己去打标签
FinBERT预训练包括字词级别的和任务级别的。
How to fine tune a pre trained base. Finger the chinese. For downstream tone class.
提示词：
- zero-shot：没有例子
- few-shot：给一些例子
- COT：please answer step by step.
输出格式不符合要求怎么办？one shot或者在提示词里说明输出格式，temprature设置为0降低随机性。
BERT没有普及开的原因：使用门槛高，需要自己微调。
TreeofThought：猜的过程去试错，避免错误的路径，强化学习
